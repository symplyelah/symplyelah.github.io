{
  "hash": "3e8c9a01c11766ff09dc20f2eab6c7fb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Topic Modeling: Turning Conversations into Strategy\"\nsubtitle: \"R Ladies Abuja\"\nauthor: Ifeoma Egbogah\nformat: \n  rladies-revealjs\nincremental: false\nembed-resources: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# The Rise of Unstructured Data {.inverse}\n\n## From Numbers to Words\n\n<div style = \"font-size:0.85em;\">\nIf you're in the field of analytics or data science, you're likely well aware that data is being produced continuously—and at an increasingly rapid pace. (You might even be tired of hearing this repeated!) While analysts are typically trained to work with structured, numeric data in table formats, a significant portion of today’s data boom involves unstructured, text-based information.\n\n>Unstructured data represents 80-90% of all new enterprise data, according to Gartner.\n\nFurthermore, it’s growing three times faster than structured data. \n\n</div>\n\n# Behind the Words {.inverse}\n## Overview\n\nIn this webinar, we will look at:\n\n* Understanding Topic Modeling\n* Importance of Topi Modeling\n* Topic Modeling Techniques\n* Implementation of Topic Modeling\n* Demo\n\n# Understanding Topic Modeling {.inverse}\n## What is Topic Modeling?\n\n<div style = \"font-size:0.85em;\">\nTopic modeling is way to identify themes/semantic patterns in a corpus (complete document).\n\nTopic modeling finds the relationships between words in the text, thereby identifying clusters of words that represent topics. \n\nIt is a like an amplified reading, a way to discover themes you may not see yourself.\n\n### Glossary:\n\n*Corpus:* Group of documents\n\n*Documents:* Newspaper, Blogpost, Tweets, Articles, Journals, Customer reviews etc.\n</div>\n\n# Importance of Topic Modeling {.inverse}\n## Key Importance of Topic Modeling\n\n<div style = \"font-size:0.70em;\">\n### *Uncovering Hidden Themes:* \nTopic modeling helps discover latent themes and patterns within unstructured text data that might otherwise be missed, providing a deeper understanding of large datasets. \n\n### *Efficient Information Retrieval and Organization:*\nIt automatically organizes and groups documents by their main themes, making it easier to find relevant information and creating a manageable structure for large text collections. \n\n### *Supporting Data-Driven Decisions:*\nBy identifying prevalent topics in customer reviews, social media, or research, organizations can make more informed decisions to improve products, services, and strategies. \n</div>\n\n## Key Importance of Topic Modeling\n\n<div style = \"font-size:0.70em;\">\n### *Automating Text Analysis:*\nIt automates the time-consuming process of manually reading and categorizing large volumes of text, increasing efficiency and reducing human effort. \n\n### *Enhancing Research and Discovery:*\nIn academia, topic modeling helps analyze research publications to reveal trends and key topics, thereby streamlining the research process and potentially leading to new discoveries. \n\n### *Improving Customer Experience:*\nBusinesses can use topic modeling to analyze customer service emails or feedback to understand major challenges and concerns, allowing for targeted improvements to service delivery. \n</div>\n\n# Topic Modeling Techniques {.inverse}\n## Latent Dirichlet Allocation (LDA)\n\n:::: columns\n::: {.column width=100%}\n\n<div style=\"text-align: center;\">\n![](./images/lda.png){width=\"90%\"}\n <p style=\"font-size:0.35em;\">Source: Introduction to Probabilistic Topic Models paper by Blei et. al</p>\n</div>\n\n:::\n::::\n\n<div style = \"font-size:0.70em;\">\nLatent Dirichlet Allocation (LDA) is one of the most common algorithms for topic modeling. It is guided by two principles, that:\n\n* Every document is a mixture of topics\n* Every topic is a mixture of words\n</div>\n\n# Implementation of Topic Modeling {.inverse}\n## Step 1\n### Data  Preparation\n<div style = \"font-size:0.85em;\">\nCollect the text data\n\nExamples:\n- Taylor Swift's lyrics (there is a `Taylor` package in R)\n\n- Spice Girls lyrics\n\n- [BBC News](https://raw.githubusercontent.com/vahadruya/Capstone-Project-Unsupervised-ML-Topic-Modelling/refs/heads/main/Input_Data/input.csv)\n\n- `Gutenbergr` package\n\n- `quanteda` package\n</div>\n\n## Step 2\n### Preprocessing\n<div style = \"font-size:0.85em;\">\nBefore modeling, we preprocess the data to put in it in a tidy format by:\n\n* Tokenization (splitting sentences into words)\n\n* Removing punctuation, numbers \n\n* Removing stop words (like the, and, is)\n\n* Find document-word counts\n</div>\n\n## Step 3\n### Create Document-term Matrix\n\n<div style = \"font-size:0.85em;\">\nA matrix that represents the frequency of each word (term) across all documents.\n\nWe can cast a one-token-per-row table into a `DocumentTermMatrix` with tidytext’s `cast_sparse()`.\n\n*- Rows* = documents;\n\n*- Columns* = terms/words.\n\n</div>\n\n## Step 4\n### Model Fitting\n\n<div style = \"font-size:0.85em;\">\nWe can then use the `LDA()` function from the `topicmodels` package to create a topic model.\n</div>\n\n## Step 5\n### Interprete and Visualise the Result\n\n<div style = \"font-size:0.85em;\">\n* Extract top keywords per topic.\n\n* Label the topics manually (e.g., “Customer Service Issues” or “Product Features”).\n\n* Visualize using tools like: `ggplot2` package\n\n</div>\n\n## Step 6\n### Apply Result\n\n<div style = \"font-size:0.85em;\">\nSummarize the result\nIdentify customer pain points\nTrack emerging trends etc\n\n</div>\n\n# Packages {.inverse}\n\n## Packages\n<div style = \"font-size:0.85em;\">\nWe will make use of the following packages\n\n- `tidyverse`\n\n- `tidytext`\n\n- `topicmodels`\n\n- `tm`\n\n</div>\n\n\n\n# Demo {.inverse}\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n## BBC News: Ever wondered what the news was really talking about beneath the headlines?\n\n<div style = \"font-size:0.85em;\">\nWe’ll be working with the [BBC News dataset](https://raw.githubusercontent.com/vahadruya/Capstone-Project-Unsupervised-ML-Topic-Modelling/refs/heads/main/Input_Data/input.csv), a collection of 2225 news articles published between 2004 and 2005, covering five major categories: **Business, Entertainment, Politics, Sport, and Technology**.\n\nThe goal of this project is to combine all the articles into one dataset and apply unsupervised topic modeling to uncover the hidden, underlying themes within the news stories. By analyzing the text, we’ll reveal the most prominent topics that dominated the public narrative during that time — all without manually assigning any labels.\n\n</div>\n\n## About the Dataset\n\n<div style = \"font-size:0.75em;\">\nThe BBC News dataset contains three key columns:\n\n- Title – the headline of each news article\n\n- Description – a brief summary or excerpt from the article\n\n- Category – the labeled topic (e.g., Business, Sport, etc.)\n\nFor this project, we focus on the Description column as the primary source of text. This is where we perform all of our text cleaning and preprocessing — removing punctuation, converting to lowercase, tokenizing, eliminating stop words, and so on.\n\nOnce the topic model is built, we use the Category column as a benchmark to evaluate how well our model’s discovered topics align with the actual labeled categories. This gives us a way to check the quality and accuracy of our model.\n</div>\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(bbc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,225\nColumns: 4\n$ ...1        <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ Title       <chr> \"India calls for fair trade rules\", \"Sluggish economy hits…\n$ Description <chr> \"india attend g7 meet seven lead industrialis nation frida…\n$ Category    <chr> \"Business\", \"Business\", \"Business\", \"Business\", \"Business\"…\n```\n\n\n:::\n:::\n\n\n\n\n## Data Cleaning\n\n<div style = \"font-size:0.85em;\">\nWe start by using the `clean_names()` function from the `janitor` package to standardize all column names (e.g., convert to lowercase, replace spaces with underscores).\n\nThen we removed duplicate rows based on the description column using `distinct(description, .keep_all = TRUE)`, so each news article description appears only once.\n\nThe data had 141 duplicate rows.\n\n</div>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbbc <- bbc |> \n  janitor::clean_names() |> \n  distinct(description, .keep_all = TRUE)\n```\n:::\n\n\n\n\n## Preprocessing\n\n<div style = \"font-size:0.85em;\">\nLet's tokenized the description column using `unnest_tokens()` to break down each news article into individual words.\n\nThen use `count(word, sort = TRUE)` to calculate the frequency of each word across the dataset and sort them in descending order.\n\n</div>\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 2\n  word      n\n  <chr> <int>\n1 mr     2799\n2 year   2655\n3 would  2401\n4 also   1997\n5 peopl  1855\n```\n\n\n:::\n:::\n\n\n\n\n## Preprocessing contd.\n<div style = \"font-size:0.85em;\">\nHow about title per word?\n</div>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbbc |> \n  unnest_tokens(word, description) |> \n  count(title, word, sort = TRUE)|> \n  head(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 3\n  title                            word      n\n  <chr>                            <chr> <int>\n1 Scissor Sisters triumph at Brits song     81\n2 Brits debate over 'urban' music  music    72\n3 Losing yourself in online gaming game     66\n4 Kilroy launches 'Veritas' party  parti    60\n5 Minimum wage increased to £5.05  wage     60\n```\n\n\n:::\n:::\n\n\n\n\n## Train a model\n\n<div style = \"font-size:0.85em;\">\nTo train a topic model using `LDA()` from the `topicmodel` package, we need to create sparse matrix from our tidy dataframe of tokens using `cast_sparse(category, word, n)`\n</div>\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1967 3458\n```\n\n\n:::\n:::\n\n\n\n<div style = \"font-size:0.85em;\">\nThis means there are 1967 titles (i.e documents) and different tokens (i.e. terms or words) in our dataset for modeling.\n\n</div>\n\n## Train a Model\n\n<div style = \"font-size:0.85em;\">\n\nA topic model like this one models:\n\n- each document as a mixture of topics\n- each topic as a mixture of words\n\nThe most important parameter when training a topic modeling is K, the number of topics. This is like k in k-means in that it is a hyperparamter of the model and we must choose this value ahead of time. \n</div>\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Explore Topic Model Result\n\n### Beta matrix\n\n<div style = \"font-size:0.85em;\">\nTo dig deeper into our topic model, we can use the `tidy()` function to convert the results into a dataframe that we can work with. This gives us two types of outputs:\n\n- *Beta matrix:* shows the probability of each word belonging to each topic (topic-word distribution)\n\n- *Gamma matrix:* shows how much each topic contributes to each document (document-topic distribution)\n\n</div>\n\n## Beta Matrix Contd.\n\n<div style=\"font-size:0.8em;\">\nWe'll start by looking at the beta matrix first.\n</div>\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 17,290 × 3\n   topic term        beta\n   <int> <chr>      <dbl>\n 1     1 song  0.00000532\n 2     2 song  0.00736   \n 3     3 song  0.00000587\n 4     4 song  0.00000655\n 5     5 song  0.00000511\n 6     1 music 0.00000532\n 7     2 music 0.0304    \n 8     3 music 0.00000587\n 9     4 music 0.00000655\n10     5 music 0.00000511\n# ℹ 17,280 more rows\n```\n\n\n:::\n:::\n\n\n\n\n## Beta Matrix Visualisation\n\n<div style = \"font-size:0.85em;\">\nSince the output is a tidy dataframe, we can easily manipulate it — including visualizing the top words with the highest probabilities for each topic.\n</div>\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](rladiesabuja_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n\n## Explore Topic Model Result\n\n### Gamma matrix\n The probability that this document is about this topic.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9,835 × 3\n   document                          topic  gamma\n   <chr>                             <int>  <dbl>\n 1 Scissor Sisters triumph at Brits      1 0.0127\n 2 Brits debate over 'urban' music       1 0.0164\n 3 Losing yourself in online gaming      1 0.0207\n 4 Kilroy launches 'Veritas' party       1 0.701 \n 5 Minimum wage increased to £5.05       1 0.0951\n 6 Nadal puts Spain 2-0 up               1 0.0229\n 7 Mobile games come of age              1 0.0190\n 8 Apple laptop is 'greatest gadget'     1 0.0115\n 9 Peer-to-peer nets 'here to stay'      1 0.0479\n10 Terror powers expose 'tyranny'        1 0.295 \n# ℹ 9,825 more rows\n```\n\n\n:::\n:::\n\n\n\n## Explore Topic Model Result Contd\n\n### Gamma matrix\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](rladiesabuja_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n## Explore Topic Model Result Contd\n\n### Gamma matrix\n\nMost common topic in the document.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](rladiesabuja_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n## Explore Topic Model Result Contd\n\n### Gamma matrix\n\nProbability of a document belonging to a topic\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](rladiesabuja_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n## Evaluation\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Evaluation\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](rladiesabuja_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n",
    "supporting": [
      "rladiesabuja_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}